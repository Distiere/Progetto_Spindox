### Progetto_Spindox
Pipeline per approfondimento e contesto di data engineer
## Fase 1

# Analisi esplorativa ed ingestione dei dati in DB
Una prima parte del lavoro è stata quella di esplorare il dataset per controllare che le domande di business potessero avere risposta. Il problema maggiore riscontrato è stato quello dell'utilizzo di nuove librerie e grande varietà e confusione nel tipo di dato. Dopo un primo approccio di trial and error per tentare di fare già una prima pulizia del dato vista la grande mole di dati si è deciso per avere una totale conversione dei dati in stringa in modo da consultare gli attributi del dataset e confermare come le domande di business siano tutte abbordabili con i dati in nostro possesso mantenenendo comunque una fedeltà assoluta ai dati forniti. Si è quindi proseguito alla ingestione del dato su db finendo con un check che ha confermato la presenza dei dati e della ingestione stessa. Potendo quindi passare ora al Silver layer.

# Implementazione di tabelle clean utilizzabili
Una volta ingeriti i dati raw si è passata ad una analisi estensiva del dato decidendo in base alle colonne disponibili quali fossero da tenere e quali invece da scartare. Notato come una diretta trasposizione da bronze a silver non ha fatto perdere integrità del dato sono passato a standardizzare tutti i dati disponiili rendendo possibile la creazione delle metriche temporali necessarie a calcolare le KPI predisposte. Una volta fatto tutto ciò prima di creare le tabelle definitive per i due CSV sono andate a fare un parsing dei timestamp in modo da avere uniformità per la futura interrogazione delle tabelle. Infine visto l'irrisorio numero di valori negativi o impossibili per quanto riguarda metriche temporali (118 su 4 milioni di record) li ho attribuiti ad errore umano e quindi sostituiti con valori nulli. Create quindi le tabelle definitive passo al mock-up di   quello che andrò a fare ed alla implementazione del gold layer

# Mock up dashboard ed iniziazione ad implementazione gold layer
Avuta una stabilità nel silver layer grazie ad un gate posto subito dopo la creazione del silver che blocca la pipeline in caso di fallimento grazie a controlli di query sql sul csv più modesto per valori reali ed un controllo a campione vista la mole del csv abbiamo la certezza che non ci siano delle incongruenze del dato da parte del bronze. Sono quindi passato ad uno studio delle librerie di streamlit che col supporto della libreria di matplot avrebbe dovuto creare la dashboard mock uo basata sul silver per essere sicuro del suo funzionamento una volta implementato il gold. Ho riscontrato delle problematiche relative più che altro all'utilizzo di streamlit non essendone pratico. Consultata la documentazione ed implementate le query necessarie ho potuto constatare la concreta possibilità della struttura del lavoro finale e quindi procedere alla scrittura del gold layer. 

# Implementazione Gold Layer e utilizzo di ultimo gate del processo ETL

Una volta superato il silver gate ho cominciato la strutturazione del gold layer basato sulle effettive domande di business finali. Le task totali sono: gold_dim_date, creata per andare a ricostruire tutte le informazioni temporali dai due csv in un'unica tabella consultabile per dashboard future, gold_dim_incident_type, creata per avere a disposizione tutti i tipi di incidenti possibili all'interno del dataset, gold_dim_location, per avere consultabile un elenco di tutti i posti citati nel datase ed infine gold_fact _incident per poter avere un effetivo resoconto di ciò che è avvenuto in ogni incidente. Dopo una prima bozza di questi task sono passato a definire l'ultimo gate del gold che va a rassicurare anche qui la chiarezza ed utilità effettiva del dato prima di poterlo lanciare sulle dashboard. Qui tramite semplici query mi sono accertato che: la tabella principale degli incidenti fosse popolata, che non ci fossero elementi nulli nel conteggio di un incidente, che l'unione tra i due csv non avesse generato dei luoghi inesistenti o nulli ed infine una misura temporale reale e non nulla o negativa. Dopo un primo test ho dovuto un attimo "rafforzare" i vari task nel gold layer per la non riuscita a primo impatto dei luoghi dove ho riscontrato 271 luoghi nulli. Per questo ho cambiato una UNION più generica in una JOIN più specifica in modo da avere record mirati e pronti all'uso. Ora il funzionamento della pipeline fino alla creazione delle tabelle finali è robusta e posso passare all'effetiva struttura della interfaccia utente e della conteinerizzazione.

# Docker e containerizzazione del processo

Avuta la stabilità delle dashboard ho semplicemente creato il dockerfile per potere containerizzare la parte di visualizzazione delle dashboard in podman in modo da essere riutilizzabile da un eventuale cliente. Ho quindi inserito nell'immagine: l'ambiente python, le librerie necessario e l'applicativo per rendere le dashboard usufruibili. Ho tenuto fuori eventuali db ed applicativi di sviluppo pensando che il db stesso potesse essere sempre in locale lato cliente mentre gli applicativi di sviluppo avrebbero reso l'utilizzo dell'intero processo troppo lento e macchioso, per ciò ho generato quindi un .dockerignore.  
Una volta testato che l'immagine di podman, installati tutti i requisiti, fosse funzionante ho dato un ultimo check da local host che le dashboard fossero usufruibili. Constatato ciò sono passato alla ideazione della parte text-to-sql.

# Text-TO-SQL

Avendo il progetto completo end-to-end con tempo di anticipo ho provato ad analizzare la situazione text-to-sql. La difficoltà è stata estrema principalmente basata sul non aver mai lavorato in un ambiente di questo tipo. Motivo per cui il supporto che ho avuto in questo settore da AI è stato abbastanza preponderante. Documentandomi ed analizzando bene le chat avute penso di aver raggiunto una conoscenza teorica di quello che a livello di codice avviene nell'applicativo. La problematica non risolta è nell'effetiva implementazione della key gemini per mancanza di conoscenza del dominio su come effettivamente usare tali key. In ogni caso il progetto dovrebbe essere completamente funzionante salvo questo mancanto finanziamento sulla parte generativa. Il chatbot è in una finestra secondaria rispetto alle dashboard consultabili con filtri che permette all'utente di fare una richieta scritta di una SELECT specifica all'interno delle dashboard. Mi sono limitato ad una richiesta "semplice" per mancata conoscenza dello strumento e punterei in caso ad ampliarla prossimamente con più tempo a disposizione per assimililarla per bene.

## Fase 2

# Impostazione del lavoro

Dopo aver ragionato e provato e rispondere alle domande presenti nella parte finale della documentazione ho fatto un ragionamento su quella che sarebbe stata la tabella di marcia.Il piano di lavoro per la Fase 2 è stato definito come segue: generare nuovi dati sintetici per simulare aggiornamenti incrementali reali, integrare i nuovi dati con quelli esistenti evitando overwrite, introdurre un data lake in formato parquet come layer di storage, rendere la pipeline idempotente e schedulabile, mantenere invariata la logica di pulizia e business (Silver/Gold) e preparare l’output per una possibile pubblicazione applicativa

# Definizione dati sintetici

Per generare i dati sintetici ho pensato di usare un ML addestrato sui dati presenti essendo dei csv molto grandi ed avendo a disposizione il silver layer che forniva tutti i dati puliti e usabili. Ho scelto il Random Forest come modello essendo quello più completo per la generazione di dati con fonte di apprendimento che sia variabili categoriche che numeriche. Per semplicità e velocità non ho fatto tuning dei parametri ed ho dato un sample di 200k valori al modello col fine di non rendere il processo troppo lungo. Inoltre notata la correlazione forte tra i due CSV e come molte delle colonne presenti nel CSV Incidents non venivano usate ed erano comunque derivate dal più grande CSV Calls invece di generare due CSV rischiando di avere dei mismatch di valori ho generato tramite ML solo Calls e generato Incidents calcolandomi le colonne che avrei poi usato per le KPI in codice deducendole dal CSV generato. Ci tengo comunque a precisare che un lavoro corretto su richiesta avrebbe portato sia il parameter tuning che una generazione mirata di tutte le colonne disponibili. Una volta addestrato il modello e generati i CSV ho fatto una concatenazione con i CSV di partenza e li ho quindi definiti come i nuovi dati cliente da dover ingerire e controllare con i dati vecchi da aggiornare.

# La nuova Pipeline

Avendo ora i dati pronti da ingerire sono andato a scrivere un nuovo file di ingestione del bronze layer che andasse a creare una cartella di data lake prendendo i CSV. Le differenze sostanziali rispetto alla prima fase sono riferite alla creazione di tabelle log che vadano a tenere conto del lavoro fatto fino ad allora e tengano conto dei confronti che vengono fatti nei vari file. Questo eprchè per andare a controllare se un file trovato in locale deve venir ingerito o meno nella pipeline ho deciso di inserire un controllo di hash del file stesso in modo che anche se ci fossero file con stesso nome ma modificati in locale vengano comunque controllati e caricati i dati nuovi nel data lake come nuovi file parquet, questo approccio rende la pipeline idempotente, permettendo di rilanciarla più volte senza introdurre duplicazioni e garantendo che solo nuovi contenuti vengano effettivamente processati. Qui le difficoltà riscontrate sono state dovute alla tipologia di file che mi ha fatto andare molto a ritroso anche nella fase 1 per nomenclatura che veniva accettata quando letta su duckdb ma non accettata come file parquet, quindi in questo caso sono dovuto andare molto più col pugno di ferro sulla stessa ortografia e stile dei nomi delle colonne che numerose volte non venivano riconosciuti e spaccavano la pipeline quando provavo ad utilizzare il silver layer per pulire i dati quindi il lavoro principale è stato quello di forzare tutti i nomi in lower snake case. Reso questo layer stabile sono passato al silver dove appunto ho riscontrato la maggior parte degli stessi problemi trovati con il bronze, l'intenzione era quella di importare molte funzioni dalla fase 1 ma vista questo differente approccio alla nomenclatura da parte dei due supporti sono dovuto andare molte volte a modificare la sanitizzazione delle colonne con numerosi tentativi che andavano sempre a trovare nuovi nomi non accettati dal codice. Reso stabile anche quest'ultimo layer la validazione ed il gold sono rimasti stabili e sono potuto passare all'orchestrazione.

# Orchestrazione

La parte di orchestrazione è partita con una problematica da affrontare, la mole di dati elevata che portava alla impossibilità di avere i dati in cloud su git ed usare quindi github actions, la soluzione momentanea è stata quella di intanto stilare in locale una run schedulata su un server preefct fisso in modo da poter assicurare l'orchestrazione tramite un worker che runnasse la nuova pipeline schedulata una volta al giorno in modo che mi potessi accertare che non solo la pipeline fosse funzionante ma che leggesse e scrivesse eventuali file nuovi in parquet, creasse le tabelle ed in caso non ci fossero file nuovi skippasse la creazione della parte di silver e gold. I problemi riscontrati sono stati principalmente con il linguaggio richiesto dai deployment di prefect che per import di funzioni richiede un percorso esteso del file. oltre questo problematiche non sono state riscontrate e sono passato a pensare su come poter effetivamente rendere questa orchestrazione operativa ed usabile con eventuale app.

# Pubblicazione app

Infine per avere una pubblicazione reale ho dovuto fare una attenta analisi di ciò che avevo svolto fino ad ora. Non potendo caricare o usare i miei dati su git avendo una mole di lavoro troppo grande ho pensato di inserire in pipeline un ultima task da eseguire a fine lavoro che andasse ad usare un nuovo db per dashboard che vada a direttamente esportare dal gold layer i dati per le kpi ed i dati più leggeri in modo da poterlo interpretare come versione trailer di quelle che sarebbero poi la versione estesa containerizzata su podman in modo che possa essere un attimo "venduta" meglio. Il problema più grande è stato l'uso di determinati formati che andavano in disaccordo con le impostazioni standard di streamlit che ho risolto andando a forzare dai requirements versioni specifiche che approvano più tipi di formato.

 