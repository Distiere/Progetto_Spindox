# Progetto_Spindox
Pipeline per approfondimento e contesto di data engineer

# Analisi esplorativa ed ingestione dei dati in DB
Una prima parte del lavoro è stata quella di esplorare il dataset per controllare che le domande di business potessero avere risposta. Il problema maggiore riscontrato è stato quello dell'utilizzo di nuove librerie e grande varietà e confusione nel tipo di dato. Dopo un primo approccio di trial and error per tentare di fare già una prima pulizia del dato vista la grande mole di dati si è deciso per avere una totale conversione dei dati in stringa in modo da consultare gli attributi del dataset e confermare come le domande di business siano tutte abbordabili con i dati in nostro possesso mantenenendo comunque una fedeltà assoluta ai dati forniti. Si è quindi proseguito alla ingestione del dato su db finendo con un check che ha confermato la presenza dei dati e della ingestione stessa. Potendo quindi passare ora al Silver layer.

# Implementazione di tabelle clean utilizzabili
Una volta ingeriti i dati raw si è passata ad una analisi estensiva del dato decidendo in base alle colonne disponibili quali fossero da tenere e quali invece da scartare. Notato come una diretta trasposizione da bronze a silver non ha fatto perdere integrità del dato sono passato a standardizzare tutti i dati disponiili rendendo possibile la creazione delle metriche temporali necessarie a calcolare le KPI predisposte. Una volta fatto tutto ciò prima di creare le tabelle definitive per i due CSV sono andate a fare un parsing dei timestamp in modo da avere uniformità per la futura interrogazione delle tabelle. Infine visto l'irrisorio numero di valori negativi o impossibili per quanto riguarda metriche temporali (118 su 4 milioni di record) li ho attribuiti ad errore umano e quindi sostituiti con valori nulli. Create quindi le tabelle definitive passo al mock-up di   quello che andrò a fare ed alla implementazione del gold layer

# Mock up dashboard ed iniziazione ad implementazione gold layer
Avuta una stabilità nel silver layer grazie ad un gate posto subito dopo la creazione del silver che blocca la pipeline in caso di fallimento grazie a controlli di query sql sul csv più modesto per valori reali ed un controllo a campione vista la mole del csv abbiamo la certezza che non ci siano delle incongruenze del dato da parte del bronze. Sono quindi passato ad uno studio delle librerie di streamlit che col supporto della libreria di matplot avrebbe dovuto creare la dashboard mock uo basata sul silver per essere sicuro del suo funzionamento una volta implementato il gold. Ho riscontrato delle problematiche relative più che altro all'utilizzo di streamlit non essendone pratico. Consultata la documentazione ed implementate le query necessarie ho potuto constatare la concreta possibilità della struttura del lavoro finale e quindi procedere alla scrittura del gold layer. 

# Implementazione Gold Layer e utilizzo di ultimo gate del processo ETL

Una volta superato il silver gate ho cominciato la strutturazione del gold layer basato sulle effettive domande di business finali. Le task totali sono: gold_dim_date, creata per andare a ricostruire tutte le informazioni temporali dai due csv in un'unica tabella consultabile per dashboard future, gold_dim_incident_type, creata per avere a disposizione tutti i tipi di incidenti possibili all'interno del dataset, gold_dim_location, per avere consultabile un elenco di tutti i posti citati nel datase ed infine gold_fact _incident per poter avere un effetivo resoconto di ciò che è avvenuto in ogni incidente. Dopo una prima bozza di questi task sono passato a definire l'ultimo gate del gold che va a rassicurare anche qui la chiarezza ed utilità effettiva del dato prima di poterlo lanciare sulle dashboard. Qui tramite semplici query mi sono accertato che: la tabella principale degli incidenti fosse popolata, che non ci fossero elementi nulli nel conteggio di un incidente, che l'unione tra i due csv non avesse generato dei luoghi inesistenti o nulli ed infine una misura temporale reale e non nulla o negativa. Dopo un primo test ho dovuto un attimo "rafforzare" i vari task nel gold layer per la non riuscita a primo impatto dei luoghi dove ho riscontrato 271 luoghi nulli. Per questo ho cambiato una UNION più generica in una JOIN più specifica in modo da avere record mirati e pronti all'uso. Ora il funzionamento della pipeline fino alla creazione delle tabelle finali è robusta e posso passare all'effetiva struttura della interfaccia utente e della conteinerizzazione.

# Docker e containerizzazione del processo

Avuta la stabilità delle dashboard ho semplicemente creato il dockerfile per potere containerizzare la parte di visualizzazione delle dashboard in podman in modo da essere riutilizzabile da un eventuale cliente. Ho quindi inserito nell'immagine: l'ambiente python, le librerie necessario e l'applicativo per rendere le dashboard usufruibili. Ho tenuto fuori eventuali db ed applicativi di sviluppo pensando che il db stesso potesse essere sempre in locale lato cliente mentre gli applicativi di sviluppo avrebbero reso l'utilizzo dell'intero processo troppo lento e macchioso, per ciò ho generato quindi un .dockerignore.  
Una volta testato che l'immagine di podman, installati tutti i requisiti, fosse funzionante ho dato un ultimo check da local host che le dashboard fossero usufruibili. Constatato ciò sono passato alla ideazione della parte text-to-sql.

# Text-TO-SQL

Avendo il progetto completo end-to-end con tempo di anticipo ho provato ad analizzare la situazione text-to-sql. La difficoltà è stata estrema principalmente basata sul non aver mai lavorato in un ambiente di questo tipo. Motivo per cui il supporto che ho avuto in questo settore da AI è stato abbastanza preponderante. Documentandomi ed analizzando bene le chat avute penso di aver raggiunto una conoscenza teorica di quello che a livello di codice avviene nell'applicativo. La problematica non risolta è nell'effetiva implementazione della key gemini per mancanza di conoscenza del dominio su come effettivamente usare tali key. In ogni caso il progetto dovrebbe essere completamente funzionante salvo questo mancanto finanziamento sulla parte generativa. Il chatbot è in una finestra secondaria rispetto alle dashboard consultabili con filtri che permette all'utente di fare una richieta scritta di una SELECT specifica all'interno delle dashboard. Mi sono limitato ad una richiesta "semplice" per mancata conoscenza dello strumento e punterei in caso ad ampliarla prossimamente con più tempo a disposizione per assimililarla per bene.